{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa059ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'einops'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'einops'"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, RandomHorizon\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03230624",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "patch_size = 16         # Patch size (P) = 16\n",
    "latent_size = 768       # Latent vector (D). ViT-Base uses 768\n",
    "n_channels = 3          # Number of channels for input images\n",
    "num_heads = 12          # ViT-Base uses 12 heads\n",
    "num_encoders = 12       # ViT-Base uses 12 encoder layers\n",
    "dropout = 0.1           # Dropout = 0.1 is used with ViT-Base & ImageNet-21k\n",
    "num_classes = 200       # Number of classes in CIFAR10 dataset\n",
    "size = 224              # Size used for training = 224\n",
    "\n",
    "epochs = 30             # Number of epochs\n",
    "base_lr = 10e-3         # Base LR\n",
    "weight_decay = 0.03     # Weight decay for ViT-Base (on ImageNet-21k)\n",
    "batch_size = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d61365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TinyImageNetDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Assuming the filename format is 'class_001.jpg'\n",
    "        label = int(img_path.split('_')[0].split('/')[-1])\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define the transformation for training data\n",
    "transform_training_data = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Path to the directory containing Tiny ImageNet images\n",
    "tiny_imagenet_path = '/path/to/tiny-imagenet-200/train'\n",
    "\n",
    "# Create a custom dataset\n",
    "tiny_imagenet_dataset = TinyImageNetDataset(root_dir=tiny_imagenet_path, transform=transform_training_data)\n",
    "\n",
    "# Create a data loader\n",
    "batch_size = 64\n",
    "trainloader = DataLoader(tiny_imagenet_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Define the class labels for Tiny ImageNet\n",
    "# Note: You may need to adjust this based on the actual labels in your dataset\n",
    "classes = [str(i) for i in range(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6477192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ce7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=patch_size, n_channels=n_channels, device=device, latent_size=latent_size, batch_size=batch_size):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = self.patch_size * self.patch_size * self.n_channels\n",
    "\n",
    "        self.linearProjection = nn.Linear(self.input_size, self.latent_size)\n",
    "\n",
    "        # Random initialization of of [class] token that is prepended to the linear projection vector.\n",
    "        self.class_token = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device)\n",
    "\n",
    "        # Positional embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, input_data):\n",
    "\n",
    "        input_data = input_data.to(self.device)\n",
    "\n",
    "        # Re-arrange image into patches.\n",
    "        patches = einops.rearrange(\n",
    "            input_data, 'b c (h h1) (w w1) -> b (h w) (h1 w1 c)', h1=self.patch_size, w1=self.patch_size)\n",
    "\n",
    "        linear_projection = self.linearProjection(patches).to(self.device)\n",
    "        b, n, _ = linear_projection.shape\n",
    "\n",
    "        # Prepend the [class] token to the original linear projection\n",
    "        linear_projection = torch.cat((self.class_token, linear_projection), dim=1)\n",
    "        pos_embed = einops.repeat(self.pos_embedding, 'b 1 d -> b m d', m=n+1)\n",
    "\n",
    "        # Add positional embedding to linear projection\n",
    "        linear_projection += pos_embed\n",
    "\n",
    "        return linear_projection\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0626217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, latent_size=latent_size, num_heads=num_heads, device=device, dropout=dropout):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Normalization layer for both sublayers\n",
    "        self.norm = nn.LayerNorm(self.latent_size)\n",
    "        \n",
    "        # Multi-Head Attention layer\n",
    "        self.multihead = nn.MultiheadAttention(\n",
    "            self.latent_size, self.num_heads, dropout=self.dropout)          \n",
    "\n",
    "        # MLP_head layer in the encoder. I use the same configuration as that \n",
    "        # used in the original VitTransformer implementation. The ViT-Base\n",
    "        # variant uses MLP_head size 3072, which is latent_size*4.\n",
    "        self.enc_MLP = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.latent_size*4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.latent_size*4, self.latent_size),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, embedded_patches):\n",
    "\n",
    "        # First sublayer: Norm + Multi-Head Attention + residual connection.\n",
    "        # We take the first element ([0]) of the returned output from nn.MultiheadAttention()\n",
    "        # because this module returns 'Tuple[attention_output, attention_output_weights]'. \n",
    "        # Refer to here for more info: https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html \n",
    "        firstNorm_out = self.norm(embedded_patches)\n",
    "        attention_output = self.multihead(firstNorm_out, firstNorm_out, firstNorm_out)[0]\n",
    "\n",
    "        # First residual connection\n",
    "        first_added_output = attention_output + embedded_patches\n",
    "\n",
    "        # Second sublayer: Norm + enc_MLP (Feed forward)\n",
    "        secondNorm_out = self.norm(first_added_output)\n",
    "        ff_output = self.enc_MLP(secondNorm_out)\n",
    "\n",
    "        # Return the output of the second residual connection\n",
    "        return ff_output + first_added_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae5871",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VitTransformer(nn.Module):\n",
    "    def __init__(self, num_encoders=num_encoders, latent_size=latent_size, device=device, num_classes=num_classes, dropout=dropout):\n",
    "        super(VitTransformer, self).__init__()\n",
    "        self.num_encoders = num_encoders\n",
    "        self.latent_size = latent_size\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = InputEmbedding()\n",
    "\n",
    "        # Create a stack of encoder layers\n",
    "        self.encStack = nn.ModuleList([EncoderBlock() for i in range(self.num_encoders)])\n",
    "\n",
    "        # MLP_head at the classification stage has 'one hidden layer at pre-training time\n",
    "        # and by a single linear layer at fine-tuning time'. For this implementation I will\n",
    "        # use what was used for training, so I'll have a total of two layers, one hidden\n",
    "        # layer and one output layer.\n",
    "        self.MLP_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.latent_size),\n",
    "            nn.Linear(self.latent_size, self.latent_size),\n",
    "            nn.Linear(self.latent_size, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, test_input):\n",
    "\n",
    "        # Apply input embedding (patchify + linear projection + position embeding)\n",
    "        # to the input image passed to the model\n",
    "        enc_output = self.embedding(test_input)\n",
    "\n",
    "        # Loop through all the encoder layers\n",
    "        for enc_layer in self.encStack:\n",
    "            enc_output = enc_layer.forward(enc_output)\n",
    "\n",
    "        # Extract the output embedding information of the [class] token\n",
    "        cls_token_embedding = enc_output[:, 0]\n",
    "\n",
    "        # Finally, return the classification vector for all image in the batch\n",
    "        return self.MLP_head(cls_token_embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = VitTransformer(num_encoders, latent_size, device, num_classes).to(device)\n",
    "\n",
    "# Betas used for Adam in paper are 0.9 and 0.999, which are the default in PyTorch\n",
    "optimizer = optim.Adam(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.LinearLR(optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16718b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    model.train().to(device)\n",
    "\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(trainloader)):\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 200 == 0:\n",
    "                print('Batch {} epoch {} has loss = {}'.format(batch_idx, epoch, running_loss/200))                \n",
    "                running_loss = 0\n",
    "\n",
    "        scheduler.step()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d01e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ima_class",
   "language": "python",
   "name": "ima_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
